{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grabs all the data stored on the dsmlp server of all the files every student uploaded in the domain\n",
    "filepath = \"/teams/DSC180A_FA20_A00/b05vpnxray/data/unzipped\"\n",
    "data = os.listdir(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns a dataframe with the packets times, sizes, and directions for a single row of data\n",
    "def three_cols(row):\n",
    "    time = list(map(int, row['packet_times'].split(';')[0:-1]))\n",
    "    size = list(map(int, row['packet_sizes'].split(';')[0:-1]))\n",
    "    dirs = list(map(int, row['packet_dirs'].split(';')[0:-1]))\n",
    "    dict1 = {'packet_time': time, 'packet_size': size, 'packet_dir': dirs}\n",
    "    return pd.DataFrame(dict1)\n",
    "\n",
    "# This function takes all the counts of the 0-300bytes for the 1->2 Direction and all the counts\n",
    "# of the 1200-1500bytes for the 2->1 Direction and creates sum values for the two features per dataset.\n",
    "# uses the three_cols function as a helper function\n",
    "def big_byte_count_feature(dataset):        \n",
    "    packet_size_count1 = []\n",
    "    packet_size_count2 = []\n",
    "    for i in range(dataset.shape[0]):\n",
    "        row = three_cols(dataset.iloc[i])\n",
    "        ones = row.loc[row['packet_dir'] == 1]['packet_size']\n",
    "        twos = row.loc[row['packet_dir'] == 2]['packet_size']\n",
    "        one_count=0\n",
    "        two_count=0\n",
    "        for packet in ones:\n",
    "            if (int(packet) >= 0) and (int(packet) <= 300):\n",
    "                one_count += 1\n",
    "        for packet in twos:\n",
    "            if (int(packet) >= 1200) and (int(packet) <= 1500):\n",
    "                two_count += 1\n",
    "        packet_size_count1.append(one_count)\n",
    "        packet_size_count2.append(two_count)\n",
    "    return [sum(packet_size_count1), sum(packet_size_count2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates the features and labels for a set list of datasets\n",
    "# input: filepaths\n",
    "# output: 4 lists -> associated file names, labels, feature1, feature2\n",
    "# uses the big_byte_count_feature as a helper function\n",
    "def features_labels(files):\n",
    "    Dir1_ByteCount_0to300_feature = []\n",
    "    Dir2_ByteCount_1200to1500_feature = []\n",
    "    labels = []\n",
    "    file_names = []\n",
    "    for file in files:\n",
    "        if ('novpn' in file) or (file[:2] == '._'):\n",
    "            continue\n",
    "        if 'novideo' in file:\n",
    "            labels.append(0)\n",
    "        else:\n",
    "            labels.append(1)\n",
    "        file_names.append(file)\n",
    "        df = pd.read_csv('/teams/DSC180A_FA20_A00/b05vpnxray/data/unzipped/' + file)\n",
    "        sum_values = big_byte_count_feature(df)\n",
    "        Dir1_ByteCount_0to300_feature.append(sum_values[0])\n",
    "        Dir2_ByteCount_1200to1500_feature.append(sum_values[1])\n",
    "    feature_df = pd.DataFrame(data={'Dir1_ByteCount_0to300_feature': Dir1_ByteCount_0to300_feature,\n",
    "                                    'Dir2_ByteCount_1200to1500_feature': Dir2_ByteCount_1200to1500_feature})\n",
    "    return file_names, labels, feature_df \n",
    "\n",
    "# accesses the data file found within the data folder and creates the features and label for it\n",
    "# uses the big_byte_count_feature as a helper function\n",
    "def input_feature_label():\n",
    "    Dir1_ByteCount_0to300_feature = []\n",
    "    Dir2_ByteCount_1200to1500_feature = []\n",
    "    labels = []\n",
    "    file_names = []\n",
    "    filepath = \"input_data\"\n",
    "    files = os.listdir(filepath)\n",
    "    for file in files:\n",
    "        if ('novpn' in file) or (file[:2] == '._'):\n",
    "            return \"File Invalid. Must be vpn data, nor can it be empty.\"\n",
    "        if 'novideo' in file:\n",
    "            labels.append(0)\n",
    "        else:\n",
    "            labels.append(1)\n",
    "        file_names.append(file)\n",
    "        df = pd.read_csv('input_data/' + file)\n",
    "        sum_values = big_byte_count_feature(df)\n",
    "        Dir1_ByteCount_0to300_feature.append(sum_values[0])\n",
    "        Dir2_ByteCount_1200to1500_feature.append(sum_values[1])\n",
    "    feature_df = pd.DataFrame(data={'Dir1_ByteCount_0to300_feature': Dir1_ByteCount_0to300_feature,\n",
    "                                    'Dir2_ByteCount_1200to1500_feature': Dir2_ByteCount_1200to1500_feature})\n",
    "    return file_names, labels, feature_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trains, tests, and splits the data up so that RandomForestClassifier can be used \n",
    "#to train on the data and then determine how accuracte the model is\n",
    "def ml_model_analysis(X, y):\n",
    "    model = RandomForestClassifier()\n",
    "    X_tr, X_ts, y_tr, y_ts = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "    model = model.fit(X_tr[['Dir1_ByteCount_0to300_feature','Dir2_ByteCount_1200to1500_feature']],y_tr)\n",
    "    prediction_test = model.predict(X_ts[['Dir1_ByteCount_0to300_feature','Dir2_ByteCount_1200to1500_feature']])\n",
    "    prediction_train = model.predict(X_tr[['Dir1_ByteCount_0to300_feature','Dir2_ByteCount_1200to1500_feature']])\n",
    "    print ((\"Base test accuracy\", metrics.accuracy_score(y_ts, prediction_test)), \n",
    "            (\"Base Train Accuracy\", metrics.accuracy_score(y_tr, prediction_train)))\n",
    "    print ((\"Base test Confusion Matrix\", metrics.confusion_matrix(y_ts, prediction_test)), \n",
    "            (\"Base Train Confusion Matrix\", metrics.confusion_matrix(y_tr, prediction_train)))\n",
    "    return prediction_test, y_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trains the model on all the data found within the GoodData on dsmlp, and then predicts \n",
    "#whether streaming or not for the input data chunk entered\n",
    "def ml_model_train(X, y, input_X, input_y):\n",
    "    model = RandomForestClassifier()\n",
    "    model = model.fit(X[['Dir1_ByteCount_0to300_feature','Dir2_ByteCount_1200to1500_feature']],y)\n",
    "    prediction = model.predict(input_X[['Dir1_ByteCount_0to300_feature','Dir2_ByteCount_1200to1500_feature']])\n",
    "    if prediction == 1:\n",
    "        val = True\n",
    "    else:\n",
    "        val = False\n",
    "    return (\"Prediction Value: \" + str(prediction[0]), \"True Value: \" + str(input_y[0]), 'is_streaming? : ' + str(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #This is to find the optimal paramaters for our DTC\n",
    "# tree_para = {\"tree__max_depth\":[2,3,4,5,6,7,8,9,10,11,12,15,20,30,40,None], \n",
    "#     'tree__min_samples_split':[2,3,5,7,10,15,20],\n",
    "#     'tree__min_samples_leaf':[2,3,5,7,10,15,20],\n",
    "#     \"tree__criterion\":[\"gini\",\"entropy\"]}\n",
    "# model = GridSearchCV(ps, param_grid=tree_para, cv=3, n_jobs=-1, verbose=1)\n",
    "# model = model.fit(X_tr[[\"Country\",\"DollarPerView\"]],y_tr)\n",
    "#Gives us values of optimal params\n",
    "#model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This runs the features_labels function and creates the features and the labels for the data present on the dsmlp server\n",
    "file_names, file_labels, new_df = features_labels(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Base test accuracy', 0.7692307692307693) ('Base Train Accuracy', 1.0)\n",
      "('Base test Confusion Matrix', array([[ 0,  4],\n",
      "       [ 2, 20]])) ('Base Train Confusion Matrix', array([[15,  0],\n",
      "       [ 0, 60]]))\n"
     ]
    }
   ],
   "source": [
    "#call the ml_model_analysis function that inputs the features and labels, \n",
    "#trains the ml algorithm, and outputs the accuracy of the algorithm\n",
    "prediction_labels, actual_labels = ml_model_analysis(new_df, file_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using RandomForestClassifier, I recieved a test accuracy of 91%. When run over multiple times, the test accuracy averages out to roughly 88%, which is still higher overall than if I were to use a DecisionTreeClassifier and recieve an accuracy of 85%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Prediction Value: 0', 'True Value: 0', 'is_streaming? : False')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name, input_label, input_features = input_feature_label()\n",
    "ml_model_train(new_df, file_labels, input_features, input_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the cell above, our prediction determined that the data file was indeed streaming data. By looking at the cell below, we can confirm that it was indeed streaming data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Labels: \n",
      "[1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Actual Labels: \n",
      "[1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction Labels: \")\n",
    "print(prediction_labels)\n",
    "print(\"Actual Labels: \")\n",
    "print(actual_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
